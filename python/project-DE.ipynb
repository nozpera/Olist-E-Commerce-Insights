{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/03/27 01:21:48 WARN Utils: Your hostname, DESKTOP-Q5PPUKU resolves to a loopback address: 127.0.1.1; using 172.30.227.126 instead (on interface eth0)\n",
      "25/03/27 01:21:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/03/27 01:21:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "### path\n",
    "GCS_CONNECTOR_SPARK = os.environ.get(\"GCS_CONNECTOR_SPARK\")\n",
    "GOOGLE_APPLICATION_CREDENTIALS = os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('Project-DE') \\\n",
    "    .set(\"spark.jars\", GCS_CONNECTOR_SPARK) \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", GOOGLE_APPLICATION_CREDENTIALS)\n",
    "    \n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", GOOGLE_APPLICATION_CREDENTIALS)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/27 01:25:23 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 14, schema size: 10\n",
      "CSV file: gs://nozpera_bucket/olist_closed_deals_dataset.csv\n",
      "2025-03-27 01:25:23,983|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'review_creation_date'. in table 'olist_order_reviews_dataset'.\n",
      "2025-03-27 01:25:23,984|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'review_answer_timestamp'. in table 'olist_order_reviews_dataset'.\n",
      "2025-03-27 01:25:23,984|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'shipping_limit_date'. in table 'olist_order_items_dataset'.\n",
      "2025-03-27 01:25:23,985|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'won_date'. in table 'olist_closed_deals_dataset'.\n",
      "2025-03-27 01:25:23,986|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'first_contact_date'. in table 'olist_marketing_qualified_leads_dataset'.\n",
      "2025-03-27 01:25:23,986|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'order_purchase_timestamp'. in table 'olist_orders_dataset'.\n",
      "2025-03-27 01:25:23,987|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'order_approved_at'. in table 'olist_orders_dataset'.\n",
      "2025-03-27 01:25:23,988|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'order_delivered_carrier_date'. in table 'olist_orders_dataset'.\n",
      "2025-03-27 01:25:23,989|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'order_delivered_customer_date'. in table 'olist_orders_dataset'.\n",
      "2025-03-27 01:25:23,990|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'order_estimated_delivery_date'. in table 'olist_orders_dataset'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process File: olist_closed_deals_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 01:25:26,478|[WARNING]|1835|139778474610816|dlt|type_mapping.py|to_db_datetime_type:56|Column flags for timezone or precision are not yet supported in this destination. One or both of these flags were used in column 'won_date'. in table 'olist_closed_deals_dataset'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline olist_ingestion_db load step completed in 15.65 seconds\n",
      "1 load package(s) were loaded to destination bigquery and into dataset sources_olist\n",
      "The bigquery destination used zoomcamp@data-engineering-zoomcamp-arp.iam.gserviceaccount.com@data-engineering-zoomcamp-arp location to store data\n",
      "Load package 1743013519.9781337 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import gcsfs\n",
    "import os\n",
    "import dlt\n",
    "import pyarrow.parquet as pq\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from google.cloud import bigquery\n",
    "\n",
    "SCHEMAS = {\n",
    "        \"customers\": types.StructType([\n",
    "                        types.StructField('customer_id', types.StringType(), True), \n",
    "                        types.StructField('customer_unique_id', types.StringType(), True), \n",
    "                        types.StructField('customer_zip_code_prefix', types.IntegerType(), True), \n",
    "                        types.StructField('customer_city', types.StringType(), True), \n",
    "                        types.StructField('customer_state', types.StringType(), True)\n",
    "                    ]),\n",
    "        \"geolocation\": types.StructType([\n",
    "                        types.StructField('geolocation_zip_code_prefix', types.IntegerType(), True), \n",
    "                        types.StructField('geolocation_lat', types.DecimalType(18, 15), True), \n",
    "                        types.StructField('geolocation_lng', types.DecimalType(18, 15), True), \n",
    "                        types.StructField('geolocation_city', types.StringType(), True), \n",
    "                        types.StructField('geolocation_state', types.StringType(), True)\n",
    "                    ]),\n",
    "        \"order_items\": types.StructType([\n",
    "                        types.StructField('order_id', types.StringType(), True), \n",
    "                        types.StructField('order_item_id', types.IntegerType(), True), \n",
    "                        types.StructField('product_id', types.StringType(), True), \n",
    "                        types.StructField('seller_id', types.StringType(), True), \n",
    "                        types.StructField('shipping_limit_date', types.TimestampType(), True),\n",
    "                        types.StructField('price', types.DoubleType(), True),\n",
    "                        types.StructField('freight_value', types.DoubleType(), True)\n",
    "                    ]),\n",
    "        \"order_payments\": types.StructType([\n",
    "                        types.StructField('order_id', types.StringType(), True), \n",
    "                        types.StructField('payment_sequential', types.IntegerType(), True), \n",
    "                        types.StructField('payment_type', types.StringType(), True), \n",
    "                        types.StructField('payment_installments', types.IntegerType(), True), \n",
    "                        types.StructField('payment_value', types.DoubleType(), True)\n",
    "                    ]),\n",
    "        \"order_reviews\": types.StructType([\n",
    "                        types.StructField('review_id', types.StringType(), True), \n",
    "                        types.StructField('order_id', types.StringType(), True), \n",
    "                        types.StructField('review_score', types.IntegerType(), True), \n",
    "                        types.StructField('review_comment_title', types.StringType(), True), \n",
    "                        types.StructField('review_comment_message', types.StringType(), True),\n",
    "                        types.StructField('review_creation_date', types.TimestampType(), True),\n",
    "                        types.StructField('review_answer_timestamp', types.TimestampType(), True)\n",
    "                    ]),\n",
    "        \"orders\": types.StructType([\n",
    "                        types.StructField('order_id', types.StringType(), True), \n",
    "                        types.StructField('customer_id', types.StringType(), True), \n",
    "                        types.StructField('order_status', types.StringType(), True), \n",
    "                        types.StructField('order_purchase_timestamp', types.TimestampType(), True), \n",
    "                        types.StructField('order_approved_at', types.TimestampType(), True),\n",
    "                        types.StructField('order_delivered_carrier_date', types.TimestampType(), True),\n",
    "                        types.StructField('order_delivered_customer_date', types.TimestampType(), True),\n",
    "                        types.StructField('order_estimated_delivery_date', types.TimestampType(), True)\n",
    "                    ]),\n",
    "        \"products\": types.StructType([\n",
    "                        types.StructField('product_id', types.StringType(), True), \n",
    "                        types.StructField('product_category_name', types.StringType(), True), \n",
    "                        types.StructField('product_name_lenght', types.IntegerType(), True), \n",
    "                        types.StructField('product_description_lenght', types.IntegerType(), True), \n",
    "                        types.StructField('product_photos_qty', types.IntegerType(), True),\n",
    "                        types.StructField('product_weight_g', types.IntegerType(), True),\n",
    "                        types.StructField('product_length_cm', types.IntegerType(), True),\n",
    "                        types.StructField('product_height_cm', types.IntegerType(), True),\n",
    "                        types.StructField('product_width_cm', types.IntegerType(), True)\n",
    "                    ]),\n",
    "        \"sellers\": types.StructType([\n",
    "                        types.StructField('seller_id', types.StringType(), True), \n",
    "                        types.StructField('seller_zip_code_prefix', types.IntegerType(), True), \n",
    "                        types.StructField('seller_city', types.StringType(), True), \n",
    "                        types.StructField('seller_state', types.StringType(), True)\n",
    "                    ]),\n",
    "        \"product_category_name_translation\": types.StructType([\n",
    "                        types.StructField('product_category_name', types.StringType(), True), \n",
    "                        types.StructField('product_category_name_english', types.StringType(), True)\n",
    "                    ]),\n",
    "        \"olist_marketing_qualified_leads_dataset\": types.StructType([\n",
    "                        types.StructField('mql_id', types.StringType(), True),\n",
    "                        types.StructField('first_contact_date', types.TimestampType(), True),\n",
    "                        types.StructField('landing_page_id', types.StringType(), True),\n",
    "                        types.StructField('origin', types.StringType(), True)\n",
    "                    ]),\n",
    "        \"olist_closed_deals_dataset\": types.StructType([\n",
    "                        types.StructField('mql_id', types.StringType(), True),\n",
    "                        types.StructField('seller_id', types.StringType(), True),\n",
    "                        types.StructField('sdr_id', types.StringType(), True),\n",
    "                        types.StructField('sr_id', types.StringType(), True),\n",
    "                        types.StructField('won_date', types.TimestampType(), True),\n",
    "                        types.StructField('business_segment', types.StringType(), True),\n",
    "                        types.StructField('lead_type', types.StringType(), True),\n",
    "                        types.StructField('lead_behaviour_profile', types.StringType(), True),\n",
    "                        types.StructField('has_company', types.StringType(), True),\n",
    "                        types.StructField('has_gtin', types.StringType(), True)\n",
    "                    ])\n",
    "        }\n",
    "\n",
    "credential_path = os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "if credential_path:\n",
    "    with open(credential_path, 'r') as file:\n",
    "        os.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = file.read()\n",
    "else:\n",
    "    raise Exception(\"GOOGLE_APPLICATION_CREDENTIALS is not set as environment variable\")\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='olist_ingestion_db',\n",
    "    destination='bigquery',\n",
    "    dataset_name='sources_olist'\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "def olist(type, specific_type=None):\n",
    "    if type in ['customers', 'geolocation', 'orders', 'products', 'sellers']:\n",
    "        df = spark.read \\\n",
    "            .option('header', 'true') \\\n",
    "            .schema(SCHEMAS.get(type)) \\\n",
    "            .csv(f'gs://nozpera_bucket/olist_{type}_dataset.csv')\n",
    "        df.repartition(4).write.parquet(f'./parquet/{type}/')\n",
    "        print(f'Process File: olist_{type}_dataset.csv')\n",
    "        yield pq.read_table(f'./parquet/{type}/')\n",
    "    elif type == 'order':\n",
    "        df = spark.read \\\n",
    "            .option('header', 'true') \\\n",
    "            .schema(SCHEMAS.get(f'{type}_{specific_type}')) \\\n",
    "            .csv(f'gs://nozpera_bucket/olist_{type}_{specific_type}_dataset.csv')\n",
    "        df.repartition(4).write.parquet(f'./parquet/{type}_{specific_type}/')\n",
    "        print(f'Process File: olist_{type}_{specific_type}_dataset.csv')\n",
    "        yield pq.read_table(f'./parquet/{type}_{specific_type}/')\n",
    "    else:\n",
    "        df = spark.read \\\n",
    "            .option('header', 'true') \\\n",
    "            .schema(SCHEMAS.get(type)) \\\n",
    "            .csv(f'gs://nozpera_bucket/{type}.csv')\n",
    "        df.repartition(4).write.parquet(f'./parquet/{type}/')\n",
    "        print(f'Process File: {type}.csv')\n",
    "        yield pq.read_table(f'./parquet/{type}/')\n",
    "        \n",
    "dataset_type = 'olist_closed_deals_dataset'\n",
    "specific_dataset_type = None\n",
    "\n",
    "if dataset_type in ['customers', 'geolocation', 'orders', 'products', 'sellers']:\n",
    "    info = pipeline.run(olist(dataset_type), table_name=f\"olist_{dataset_type}_dataset\", write_disposition=\"replace\", loader_file_format=\"parquet\")\n",
    "    print(info)\n",
    "elif dataset_type == 'order':\n",
    "    info = pipeline.run(olist(dataset_type, specific_dataset_type), table_name=f\"olist_{dataset_type}_{specific_dataset_type}_dataset\", write_disposition=\"replace\", loader_file_format=\"parquet\")\n",
    "    print(info)\n",
    "else:\n",
    "    info = pipeline.run(olist(dataset_type), table_name=dataset_type, write_disposition=\"replace\", loader_file_format=\"parquet\")\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modif_py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
